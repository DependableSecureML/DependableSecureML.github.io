<html>

<head>
<link rel="stylesheet" type="text/css" href="css/style.css" title="style1">
<link rel="icon" type="image/ico" href="images/favico.ico">
<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,300,700' rel='stylesheet' type='text/css'>
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,700" rel="stylesheet">
<meta name="description" content="Dependable and Secure Machine Learning Workshop">
<meta name="keywords" content="dependable machine learning, secure machine learning, DSN workshops, DMSL">
<meta http-equiv="Content-Type" content="text/html">
<title>DSML 2023: Dependable and Secure Machine Learning</title>
</head>

<body bgcolor="#FFFFFF">
<div id="container">
<table width="100%">
<tr valign="top">
<ul class="topnav">
<li><a href="index.html">Home</a></li>
<li><a href="cfp.html">Call for Papers<br></a></li>
<li><a href="committee.html">Committees</a></li>
<li><a class="active" href="program.html">Workshop Program</a></li>
<li><a href="https://dsn2023.dei.uc.pt/">Venue</a></li>
<!--li><a href="https://dsn2023.dei.uc.pt/">Registration</a></li-->
<li><a href="previous.html">Previous Workshop Editions</a></li>
</ul>
</tr>
<tr>
<td><br></td>
</tr>
<tr valign="top">
<td width="80%" valign="top" style="padding-bottom:20px">
<h1>DSML 2023<br>Dependable and Secure Machine Learning</h1>
</td>
<td valign="top" align="center">
<img src="images/2023.png" width="60%"><br>
</td>
</tr>

<tr valign="top">
<td width="70%" valign="top">

<h2>Workshop Program - Tuesday, June 27, 2023</h2>

<table cellpadding="0" cellspacing="0" class="c12" width="1200">
<tbody>
<tr>
<td class="c10 c11"><span class="c2">9:00 WEST</span></td>
<td class="c9 c11"><span class="c8 c2">Welcome to DSN-DSML 2023</span><br>
<!--span class="c2 c3 d2">Saurabh Jha, IBM Research</span--><br />
</td>
</tr>

<tr>
<td class="c7 c13"><span class="c2"> </span></td>
<td class="c9 c13"><span class="c8 c2">Session 1: Keynote Talk</span><br>
</td>
</tr>

<tr>
<td class="c10"><span class="c2"></span>
<span class="c2">09:15 WEST</span>
<br><br><span class="c2">10:15 WEST</span>
</td>
<td class="c9">
<span class="c8 c2"><a href="#keynotes">Understanding and improving the reliability of Machine Learning accelerators: from GPUs to TPUs and FPGAs</a></span><br>
<span class="c2 c3 d2"><a href="https://www.inf.ufrgs.br/~prech/Paolo_Rech/home.html">Paolo Rech</a>, University of Trento/UFRGS<br />
Q&A
</span>
</td>
</tr>

<tr>
<td class="c7"><span class="c2">10:30 WEST</span></td>
<td class="c9 c11"><span class="c8 c2">Coffee Break</span></td>
</tr>

<tr>
<td class="c7 c13"><span class="c2"> </span></td>
<td class="c9 c13"><span class="c8 c2">Session 2: Dependable and Secure Image Classification</span>
<br>
<!--span class="c2 c3 d2">Session Chair: Matthew Jagielski</span-->
</td>
</tr>
<tr>
<td class="c10"><span class="c2"></span>
<span class="c2">11:00 WEST</span>
<br><br><br><br><span class="c2">11:30 WEST</span>
<br><br><br><br><span class="c2">12:00 WEST</span>
</td>
<td class="c9">
<span class="c8 c2"><a>Adversarial Patch Detection and Mitigation by Detecting High Entropy Regions</a></span><br />
<span class="c2 c3 d2">Niklas Bunzel (Fraunhofer SIT), Ashim Siwakoti (Fraunhofer SIT), Gerrit Klause (Fraunhofer SIT)</span><br />

<br style="margin-bottom:1pt;"><br />

<span class="c8 c2"><a>IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness</a></span><br />
<span class="c2 c3 d2">Xiaoyun Xu (Radboud University), Guilherme Perin (Radboud University), Stjepan Picek (Radboud University & Delft University of Technology)</span><br />
<br style="margin-bottom:1pt;"><br />

<span class="c8 c2"><a>A Concise Analysis of Pasting Attacks and their Impact on Image Classification</a></span><br>
<span class="c2 c3 d2">Niklas Bunzel (Fraunhofer SIT), Lukas Graner (Fraunhofer SIT)</span><br />
<br style="margin-bottom:1pt;"><br />

</td>
</tr>

<tr>
<td class="c7"><span class="c2">12:30 WEST</span></td>
<td class="c9 c11"><span class="c8 c2">Lunch Break</span></td>
</tr>

<tr>
<td class="c7 c13"><span class="c2"> </span></td>
<td class="c9 c13"><span class="c8 c2">Session 3: Keynote Talk</span><br>
</td>
</tr>
	
<tr>
<td class="c10"><span class="c2"></span>
<span class="c2">14:00 WEST</span>
<br><br><span class="c2">15:00 WEST</span>
</td>
<td class="c9">
<span class="c8 c2"><a href="#keynotes">Evaluating Privacy in Machine Learning</a> </span><br>
<span class="c2 c3 d2"><a href="https://ajpaverd.org">Andrew Paverd</a>,
Microsoft Security Response Center <br>
Q&A
</span>
</td>
</tr>


<tr>
<td class="c7 c13"><span class="c2"> </span></td>
<td class="c9 c13"><span class="c8 c2">Session 4: Other Dependable and Secure ML Systems</span>
<br>
<!--span class="c2 c3 d2">Session Chair: Matthew Jagielski</span-->
</td>
</tr>
<tr>
<td class="c10"><span class="c2"></span>
<span class="c2">16:30 WEST</span>
<br><br><br><br><span class="c2">17:00 WEST</span>
<br><br><br><br><span class="c2">17:30 WEST</span>
<br><br><br><br><span class="c2">18:00 WEST</span>
</td>
<td class="c9">
<span class="c8 c2"><a>FADO: A Federated Learning Attack and Defense Orchestrator</a></span><br />
<span class="c2 c3 d2">Filipe Rodrigues (LaSIGE, Faculdade de Ciencias, Universidade de Lisboa), Rodrigo Simoes(LaSIGE, Faculdade de Ciencias, Universidade de Lisboa), Nuno Neves (LaSIGE, Faculdade de Ciencias, Universidade de Lisboa)</span><br />

<br style="margin-bottom:1pt;"><br />

<span class="c8 c2"><a>Enhancing the Reliability of Perception Systems using N-version Programming and Rejuvenation</a></span><br />
<span class="c2 c3 d2">Julio Mendonca (University of Luxembourg), Fumio Machida (University of Tsukuba), Marcus Volp (SnT, University of Luxembourg)</span><br />
<br style="margin-bottom:1pt;"><br />

<span class="c8 c2"><a>Discussion and Closing Remarks</a></span><br>
<!--span class="c2 c3 d2">todo</span--><br />
<br style="margin-bottom:1pt;"><br />

<span class="c8 c2"><a>(Virtual) Revenue Maximization of a Slice Broker in the Presence of Byzantine Faults</a></span><br>
<span class="c2 c3 d2">Md Muhidul Khan (University of Stavanger), Gianfranco Nencioni (University of Stavanger)</span><br />
<br style="margin-bottom:1pt;"><br />

</td>
</tr>

</tbody>
</table>
</td>
</tr>
<tr>
<td><br></td>
</tr>

<tr valign="top">
<td width="70%" valign="top">


<h2><a id="keynotes">Keynotes</a></h2>
<p>


<table border="0" cellpadding="0" cellspacing="0">
<tbody font-weight: bold;>
<tr>
<img src="https://www.inf.ufrgs.br/~prech/Paolo_Rech/home_files/shapeimage_3.png" width="15%"><br>
<b>Understanding and improving the reliability of Machine Learning accelerators: from GPUs to TPUs and FPGAs</b><br>
<a href="https://www.inf.ufrgs.br/~prech/Paolo_Rech/home.html"> Paolo Rech</a>, University of Trento/UFRGS
<br>
<br>
<b>Abstract:</b>  Machine Learning (ML) is ubiquitous and its potential is attractive in many applications, from driverless cars to robotics, medicine, and even deep space exploration. For instance, NASA and ESA are willing to add self-driving capabilities to their rovers and to improve the features of satellites, such as image processing, debrits detention, cloud screening, and even pose estimation during docking. Several low-cost and low-power accelerators for ML execution have recently been introduced on the market. This is the case of embedded Graphics Processing Units (GPUs), Tensor Processing Units (TPUs) and System-on-Chips with an FPGA fabric. By optimizing the data transfer and relying on dedicated functional units design, ML accelerators efficiency outperforms traditional computing devices. The power efficiency and flexibility of ML accelerators can potentially further extend the use of ML even in power-constraint projects. Clearly before integrating a ML accelerator in the project it is fundamental to estimate its reliability and to understand how a hardware fault can propagate and modify the neural network output. Since each accelerator has a specific architecture, the manifestation of the hardware fault in software is likely to be device-dependent and needs to be evaluated.

In the talk, after a brief description of radiation effects at physical level, we will experimentally investigate the reliability of ML accelerators,  show if and why a neutron-induced corruption can modify the autonomous vehicles behaviors, and discuss the implications of these corruptions for the adoption in mission-critical applications.
The presented evaluation, to be accurate and realistic, is based on the combination of accelerated beam experiments and, when available, fault injection. This combination allows us to have a realistic evaluation of the error rate, distinguish between tolerable errors and critical errors, and to design efficient and effective hardening solutions. By hardening only critical error sources, by modifying some of the key layers in a neural network, by taking advantage of novel architectural solutions, or by applying algorithm protection, we are able to significantly increase the reliability of the application (up to 85% error detection) without unnecessary overhead (overhead as low as 0.1%).

<br>
<br>
<b>Speaker Bio:</b> Paolo Rech received his master and Ph.D. degrees from Padova University, Padova, Italy, in 2006 and 2009, respectively. He was then a Post Doc at LIRMM in Montpellier, France. Since 2022 Paolo is an associate professor at Universit√† di Trento, in Italy and since 2012 he is an associate professor at UFRGS in Brazil. He is the 2019 Rosen Scholar Fellow at the Los Alamos National Laboratory, he received the 2020 impact in society award from the Rutherford Appleton Laboratory, UK. In 2020 Paolo was awarded the Marie Curie Fellowship at Politecnico di Torino, in Italy. His main research interests include the evaluation and mitigation of radiation-induced effects in autonomous vehicles for automotive applications and space exploration, in large-scale HPC centers, and quantum computers.

<br>
<br>
</tr>
<tr>
<img src="images/Andrew Paverd.jpg" width="15%"><br>
<b>Evaluating Privacy in Machine Learning</b><br>
<a href="https://ajpaverd.org">Andrew Paverd</a>, Microsoft Security Response Center
<br>
<br>
<b>Abstract:</b> The use of domain-specific private data can add significant value to ML models, but also requires us to ensure that this data is adequately protected, even from users of the model. Privacy-preserving machine learning has been the focus of a significant body of research, with several tools and techniques now available. However, in real-world deployments, various practical questions may still arise: What specific threats are we mitigating? What level of privacy is sufficient? How do we know we have achieved the desired privacy level? In this talk, I will discuss recent work on the theme of evaluating privacy in ML, ranging from the use of "Privacy Games" to formalize and reason about specific risks, through to techniques for empirically estimating the level of privacy achieved.

<br>
<br>
<b>Speaker Bio:</b> Andrew Paverd (<a href="https://ajpaverd.org/">https://ajpaverd.org/</a>) is a Principal Research Manager in the Microsoft Security Response Center (MSRC), where he leads the strategic research initiative on AI Security & Privacy. In collaboration with researchers from across Microsoft, he has been working on tools and techniques to measure and mitigate privacy risks in machine learning. His research interests also include web and systems security. Prior to joining Microsoft, he was a Fulbright Cyber Security Scholar at the University of California, Irvine, and a Research Fellow in the Secure Systems Group at Aalto University. He received his DPhil from the University of Oxford in 2016.


<br>
<br>
</tr>
</tbody>
</table>
</td>
<td rowspan=3>


<!--<center>
<img src="images/alzetteriver.jpg" width="55%"><br>
<font size="-1" color="#CCCCCC">Image credit: Wolfgang Staugt</a></font>
</center>
</td>
</tr>
-->

</table>
</div>
</body>

</html>
